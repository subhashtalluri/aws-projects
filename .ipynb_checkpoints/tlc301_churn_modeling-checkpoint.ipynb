{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customer churn prediction with Network and Customer data using Amazon SageMaker XGBoost\n",
    "_**Supervised Learning with Gradient Boosted Trees: A Binary Prediction Problem With Unbalanced Classes**_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Background\n",
    "Customer churn is a business challenge that all Communication Service Providers encounter on a daily basis. The goal of this machine learning notebook is to look at historical data from the network, subscribed products, usage, and pricing to predict the conditions under which a particular customer is likely to churn.The steps include:\n",
    "\n",
    "* Preparing your Amazon SageMaker notebook\n",
    "* Downloading data from S3 into Amazon SageMaker\n",
    "* Investigating and transforming the data so that it can be fed to Amazon SageMaker algorithms\n",
    "* Estimating a model using the Gradient Boosting algorithm\n",
    "* Evaluating the effectiveness of the model\n",
    "* Setting the model up to make on-going predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1 - Uncomment and run cell 1 only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 01\n",
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Import all required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 02 - Import libraries\n",
    "import awswrangler as wr\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker \n",
    "import zipfile                                    # Amazon SageMaker's Python SDK provides many helper functions\n",
    "import getpass\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Change bucket name to the bucket in your S3. Set path as below with your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 03\n",
    "bucket='tlc301assets' #It should be of the form tlc301-account_number\n",
    "path = f\"s3://tlc301assets/\"\n",
    "data = 'joined'\n",
    "prefix = 'prepared'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4 - Read parquet data that was an output from the previous section of the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>vmail_plan</th>\n",
       "      <th>vmail_message</th>\n",
       "      <th>day_mins</th>\n",
       "      <th>day_calls</th>\n",
       "      <th>day_charge</th>\n",
       "      <th>eve_mins</th>\n",
       "      <th>...</th>\n",
       "      <th>partition</th>\n",
       "      <th>location5gcell</th>\n",
       "      <th>avg_5g_ran_health_index_mwc_1</th>\n",
       "      <th>avg_5g_availability_mwc_1</th>\n",
       "      <th>avg_5g_accessibility_mwc_1</th>\n",
       "      <th>avg_5g_mobility_mwc_1</th>\n",
       "      <th>avg_5g_retainability_mwc_1</th>\n",
       "      <th>avg_5g_cell_downlink_avg_throughput_den_huaw</th>\n",
       "      <th>avg_5g_cell_uplink_avg_throughput_den_huaw</th>\n",
       "      <th>avg_5g_user_downlink_avg_throughput_den_huaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>778</td>\n",
       "      <td>970-7188</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>11.809295</td>\n",
       "      <td>5</td>\n",
       "      <td>7.889440</td>\n",
       "      <td>7.582861</td>\n",
       "      <td>...</td>\n",
       "      <td>partition-limit-500000-offset-0</td>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>0.979645</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.91785</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>6.950833</td>\n",
       "      <td>26.417546</td>\n",
       "      <td>6.721416e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>777</td>\n",
       "      <td>596-5803</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>10.994388</td>\n",
       "      <td>3</td>\n",
       "      <td>10.213156</td>\n",
       "      <td>6.963251</td>\n",
       "      <td>...</td>\n",
       "      <td>partition-limit-500000-offset-0</td>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>0.979645</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.91785</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>6.950833</td>\n",
       "      <td>26.417546</td>\n",
       "      <td>6.721416e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>678</td>\n",
       "      <td>975-1551</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>100</td>\n",
       "      <td>7.801079</td>\n",
       "      <td>2</td>\n",
       "      <td>6.190318</td>\n",
       "      <td>6.651484</td>\n",
       "      <td>...</td>\n",
       "      <td>partition-limit-500000-offset-0</td>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>0.979645</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.91785</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>6.950833</td>\n",
       "      <td>26.417546</td>\n",
       "      <td>6.721416e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>798</td>\n",
       "      <td>468-9308</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>10.455195</td>\n",
       "      <td>3</td>\n",
       "      <td>4.531812</td>\n",
       "      <td>3.691651</td>\n",
       "      <td>...</td>\n",
       "      <td>partition-limit-500000-offset-0</td>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>0.979645</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.91785</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>6.950833</td>\n",
       "      <td>26.417546</td>\n",
       "      <td>6.721416e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>676</td>\n",
       "      <td>344-9464</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>5.635760</td>\n",
       "      <td>1</td>\n",
       "      <td>7.729696</td>\n",
       "      <td>6.227893</td>\n",
       "      <td>...</td>\n",
       "      <td>partition-limit-500000-offset-0</td>\n",
       "      <td>BASAKLAPUCEBN</td>\n",
       "      <td>0.979645</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.91785</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>6.950833</td>\n",
       "      <td>26.417546</td>\n",
       "      <td>6.721416e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        location area_code     phone intl_plan vmail_plan  vmail_message  \\\n",
       "0  BASAKLAPUCEBN       778  970-7188        no         no              0   \n",
       "1  BASAKLAPUCEBN       777  596-5803       yes         no              0   \n",
       "2  BASAKLAPUCEBN       678  975-1551       yes        yes            100   \n",
       "3  BASAKLAPUCEBN       798  468-9308        no         no              0   \n",
       "4  BASAKLAPUCEBN       676  344-9464        no         no              0   \n",
       "\n",
       "    day_mins  day_calls  day_charge  eve_mins  ...  \\\n",
       "0  11.809295          5    7.889440  7.582861  ...   \n",
       "1  10.994388          3   10.213156  6.963251  ...   \n",
       "2   7.801079          2    6.190318  6.651484  ...   \n",
       "3  10.455195          3    4.531812  3.691651  ...   \n",
       "4   5.635760          1    7.729696  6.227893  ...   \n",
       "\n",
       "                         partition  location5gcell  \\\n",
       "0  partition-limit-500000-offset-0   BASAKLAPUCEBN   \n",
       "1  partition-limit-500000-offset-0   BASAKLAPUCEBN   \n",
       "2  partition-limit-500000-offset-0   BASAKLAPUCEBN   \n",
       "3  partition-limit-500000-offset-0   BASAKLAPUCEBN   \n",
       "4  partition-limit-500000-offset-0   BASAKLAPUCEBN   \n",
       "\n",
       "   avg_5g_ran_health_index_mwc_1  avg_5g_availability_mwc_1  \\\n",
       "0                       0.979645                   0.999954   \n",
       "1                       0.979645                   0.999954   \n",
       "2                       0.979645                   0.999954   \n",
       "3                       0.979645                   0.999954   \n",
       "4                       0.979645                   0.999954   \n",
       "\n",
       "   avg_5g_accessibility_mwc_1  avg_5g_mobility_mwc_1  \\\n",
       "0                     0.91785               0.995625   \n",
       "1                     0.91785               0.995625   \n",
       "2                     0.91785               0.995625   \n",
       "3                     0.91785               0.995625   \n",
       "4                     0.91785               0.995625   \n",
       "\n",
       "   avg_5g_retainability_mwc_1  avg_5g_cell_downlink_avg_throughput_den_huaw  \\\n",
       "0                     0.99983                                      6.950833   \n",
       "1                     0.99983                                      6.950833   \n",
       "2                     0.99983                                      6.950833   \n",
       "3                     0.99983                                      6.950833   \n",
       "4                     0.99983                                      6.950833   \n",
       "\n",
       "   avg_5g_cell_uplink_avg_throughput_den_huaw  \\\n",
       "0                                   26.417546   \n",
       "1                                   26.417546   \n",
       "2                                   26.417546   \n",
       "3                                   26.417546   \n",
       "4                                   26.417546   \n",
       "\n",
       "  avg_5g_user_downlink_avg_throughput_den_huaw  \n",
       "0                                 6.721416e+06  \n",
       "1                                 6.721416e+06  \n",
       "2                                 6.721416e+06  \n",
       "3                                 6.721416e+06  \n",
       "4                                 6.721416e+06  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 04\n",
    "df = wr.s3.read_parquet(path+data, dataset=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data Exploration - Use Amazon QuickSignt on next section\n",
    "Please see next section where we do data exploration on Amazon QuickSight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 -  Data Transformation\n",
    "Let's transform the data to make it ready for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 05\n",
    "\n",
    "# Make Target Churn column binary\n",
    "#change True. to 1 and False. to 0\n",
    "df['churn'] = df['churn'].replace(regex=r'True.' , value='1')\n",
    "df['churn'] = df['churn'].replace(regex=r'False.' , value='0')\n",
    "df['phone'] = df['phone'].replace(regex=r'-' , value='')\n",
    "\n",
    "# Drop another location column that is redundant \n",
    "df = df.drop('location5gcell', axis=1)\n",
    "\n",
    "# One hot encoding\n",
    "df = pd.get_dummies(df, columns=['location', 'intl_plan', 'vmail_plan'])\n",
    "\n",
    "# Drop another location column that is redundant \n",
    "df = df.drop('partition', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 6 -  Split into Test, Train and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 06\n",
    "\n",
    "# Copy the original dataframe into a new dataframe\n",
    "model_data = df.copy() \n",
    "\n",
    "# Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting.  Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given.  This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown.  These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data.  There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc.  For our purposes, we'll simply randomly split the data into 3 uneven groups.  The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 -  Create tranining matrix for XGBoost algorithm\n",
    "Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  For this example, we'll stick to CSV.  Note that the first column must be the target variable and the CSV should not include headers.  Also, notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 07\n",
    "\n",
    "# Export data to your local EBS volume.\n",
    "pd.concat([train_data['churn'], train_data.drop(['churn'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['churn'], validation_data.drop(['churn'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data['churn'], test_data.drop(['churn'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 08\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8 - Training\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model, trained using Amazon SageMaker's managed, distributed training framework.\n",
    "\n",
    "First we'll need to specify the ECR container location for Amazon SageMaker's implementation of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 09\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:27:39 Starting - Starting the training job...\n",
      "2022-11-18 18:28:07 Starting - Preparing the instances for trainingProfilerReport-1668796059: InProgress\n",
      ".........\n",
      "2022-11-18 18:29:26 Downloading - Downloading input data...\n",
      "2022-11-18 18:30:06 Training - Downloading the training image......\n",
      "2022-11-18 18:31:06 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-11-18:18:30:59:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-11-18:18:30:59:INFO] File size need to be processed in the node: 2.18mb. Available memory size in the node: 8837.42mb\u001b[0m\n",
      "\u001b[34m[2022-11-18:18:30:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:30:59] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:30:59] 3500x148 matrix with 518000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2022-11-18:18:30:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:30:59] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:30:59] 1000x148 matrix with 148000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.113714#011validation-error:0.144\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.095143#011validation-error:0.119\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.086286#011validation-error:0.123\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.078857#011validation-error:0.1\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.072286#011validation-error:0.097\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.069143#011validation-error:0.088\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.063714#011validation-error:0.08\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.056857#011validation-error:0.076\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.051714#011validation-error:0.066\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.050571#011validation-error:0.061\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.046#011validation-error:0.054\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.045143#011validation-error:0.056\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.043429#011validation-error:0.055\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.038286#011validation-error:0.047\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.036571#011validation-error:0.049\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.033714#011validation-error:0.046\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.032857#011validation-error:0.046\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.030286#011validation-error:0.044\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.028571#011validation-error:0.042\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.028571#011validation-error:0.043\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.026571#011validation-error:0.04\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.026286#011validation-error:0.037\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.023714#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.023429#011validation-error:0.034\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.022857#011validation-error:0.034\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.022#011validation-error:0.033\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.020857#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.021429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.020571#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.020286#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.020571#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 14 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.020571#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.020286#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.018#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.018286#011validation-error:0.033\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.018286#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 14 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.018571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.016571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.017143#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.017143#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.016571#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.015429#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.014857#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.014571#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.014571#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.014571#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.014286#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 16 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.014#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.013429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.013429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.013429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.013143#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.013714#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.013714#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 12 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.012571#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.012571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.013714#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.013429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.013429#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.012857#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.012857#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.012857#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:30:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.012857#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.012857#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.012857#011validation-error:0.032\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.012571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.012571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.012571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.012571#011validation-error:0.031\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.012#011validation-error:0.03\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.012#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.012#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.012#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.012#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.011714#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.011429#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.011429#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.011143#011validation-error:0.028\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.011143#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.011143#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.011143#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\u001b[34m[18:31:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.011429#011validation-error:0.029\u001b[0m\n",
      "\n",
      "2022-11-18 18:31:26 Completed - Training job completed\n",
      "Training seconds: 128\n",
      "Billable seconds: 128\n"
     ]
    }
   ],
   "source": [
    "# cell 11\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/output'.format(bucket),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9 - Hosting\n",
    "Now that we've trained the `xgboost` algorithm on our data, let's deploy a model that's hosted behind a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# cell 12\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10 - Model Evaluation\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply comparing actual to predicted values.  In this case, we're simply predicting whether the customer subscribed to a term deposit (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint.  Our data is currently stored as NumPy arrays in memory of our notebook instance.  To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "*Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13\n",
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14\n",
    "def predict(data, predictor, rows=500 ):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.drop(['churn'], axis=1).to_numpy(), xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions  0.0  1.0\n",
       "actuals              \n",
       "0            256    6\n",
       "1              5  233"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 15\n",
    "pd.crosstab(index=test_data['churn'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, of the ~500 potential customers, we predicted 247 would churn and 241 of them actually did.  We also had 253 customers who we predicted would not churn, out of which 14.  This is less than desirable, but the model can (and should) be tuned to improve this.  Most importantly, note that with minimal effort, our model produced accuracies similar to those published [here](http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf).\n",
    "\n",
    "_Note that because there is some element of randomness in the algorithm's subsample, your results may differ slightly from the text written above._\n",
    "\n",
    "Hyperparameter optimization and balancing of the classes is typically recommended to improve model performance. This is not part of today's session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This example analyzed a relatively small dataset, but utilized Amazon SageMaker features such as distributed, managed training and real-time model hosting, which could easily be applied to much larger problems.  In order to improve predictive accuracy further, we could tweak value we threshold our predictions at to alter the mix of false-positives and false-negatives, or we could explore techniques like hyperparameter tuning.  In a real-world scenario, we would also spend more time engineering features by hand and would likely look for additional datasets to include which contain customer information not available in our initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
